# Distributed Locks with Redis
Redis를 활용한 분산 락

분산 락은 서로 다른 프로세스들이 공유 자원을 상호 배타적으로 사용해야 하는 많은 환경에서 매우 유용합니다.

Redis를 가지고 분산 락 매니저를 구현한 라이브러리들과 설명한 블로그들이 있지만, 각자 다른 접근법을 가지고 있고, 구현이 간단하지만
복잡한 방식보다는 낮은 수준의 보장을 제공하고 있습니다. 

이 페이지에서는 Redis를 활용한 분산 락을 구현하는 좀 더 정형화된 알고리즘을 설명합니다. 이 알고리즘은 **RedLock**이라고 합니다.
RedLock은 DLM(Distributed Lock Manager)를 단일 인스턴스 방식보다 안전하게 구현합니다. 이 알고리즘이 커뮤니티를 통해 분석되고, 피드백을 받으며,
대체안들과 좀 더 나은 방안을 위한 시발점이 되기를 바랍니다.

## Implementations
***
알고리즘을 설명하기 전에, 참고용으로 사용할 수 있는 이미 공개된 구현체들의 링크를 소개하겠습니다.
- [**RedLock-rb**](https://github.com/antirez/redlock-rb) *(Ruby 구현체)*  
  RedLock-rb 중에는 손쉬운 배포를 위한 젬(gem)을 추가한 포크 버전도 있습니다.
- [**RedisQueuedLocks**](https://github.com/0exp/redis_queued_locks) *(Ruby 구현체)*
- [**RedLock-py**](https://github.com/SPSCommerce/redlock-py) *(Python 구현체)*
- [**Pottery**](https://github.com/brainix/pottery) *(Python 구현체)*
- [**Aioredlock**](https://github.com/joanvila/aioredlock) *(Asyncio 기반 Python 구현체)*
- [**RedisMutex**](https://github.com/nicolasff/phpredis/blob/master/examples/RedisMutex.php) *(Redis 확장과 Predis 클라이언트 모두 지원하는 PHP 구현체)*
- [**Redlock-php**](https://github.com/php-lock/lock) *(PHP 구현체)*
- [**cheprasov/php-redis-lock**](https://github.com/Cheprasov/php-redis-lock) *(PHP용 락 라이브러리)*
- [**rtckit/react-redlock**](https://github.com/rtckit/react-redlock) *(Async PHP 구현체)*
- [**Redsync**](https://github.com/go-redsync/redsync) *(Go 구현체)*
- [**Redisson**](https://github.com/redisson/redisson) *(Java 구현체)*
- [**Redis::DistLock**](https://metacpan.org/pod/Redis::DistLock) *(Perl 구현체)*
- [**Redlock-cpp**](https://github.com/SPSCommerce/redlock-cpp) *(C++ 구현체)*
- [**Redis-plus-plus**](https://github.com/sewenew/redis-plus-plus) *(C++ 구현체)*
- [**Redlock-cs**](https://github.com/samcook/Redlock-cs) *(C#/.NET 구현체)*
- [**RedLock.net**](https://github.com/samcook/RedLock.net) *(C#/.NET 구현체, 비동기 및 락 연장 기능 지원)*
- [**Redlock4Net**](https://github.com/autoscout24/Redlock4Net) *(C# .NET 구현체)*
- [**node-redlock**](https://github.com/mike-marcacci/node-redlock) *(Node.js 구현체, 락 연장 기능 지원)*
- [**Deno DLM**](https://github.com/jacobwgillespie/deno_dlm) *(Deno 구현체)*
- [**Rslock**](https://github.com/bsm/rslock) *(Rust 구현체, 비동기 및 락 연장 기능 지원)*

## Safety and Liveness Guarantees
***
분산 락을 효과적으로 사용하기 위해 반드시 보장되어야 하는 3가지 요소를 바탕으로 분산 락을 모델링해보겠습니다.
1. 안전성: 상호 배제. 같은 시점에 하나의 클라이언트만 락을 소유할 수 있다.
2. 생존성 A: 데드락 없음. 자원을 점유한 클라이언트가 크래시되거나 네트워크 분할되어도 항상 락을 획득할 수 있어야 한다.
3. 생존성 B: 장애 허용. Redis 노드의 과반수가 살아있는 한 클라이언트는 락을 해제하고 점유할 수 있어야 한다.

## Why Failover-based Implementations Are Not Enough
***
분산 락을 개선하기 전에, 대부분의 Redis 기반의 분산 락 라이브러리들의 현재 상황을 알아봅시다.

Redis를 통해 자원에 락을 생성하는 가장 간단한 방법은 Redis 인스턴스에 키를 생성하면 됩니다. Redis의 TTL을 활용하여 클라이언트가 점유하고
있는 자원에 대한 점유를 해제할 때, 키를 삭제합니다.

이 방식은 잘 작동하는 것처럼 보이지만, 문제가 있습니다: Redis 마스터 노드가 내려가면 이는 SPOF가 됩니다. 이에 대해 레플리카를 추가해 마스터가
사용불가 일시 레플리카를 대신 사용할 수 있습니다. 하지만, 이 방법은 현실적으로 불가능합니다. 그 이유는 Redis는 비동기적 복제를 통해 복제본을
최신화시키기 때문에 상호 배제를 보장할 수 없습니다.

이 모델은 경쟁 상태가 존재합니다:
1. 클라이언트 A가 마스터에서 락을 획득한다.
2. 복제본에 락에 대한 키가 전해지기 전에 마스터가 다운된다.
3. 복제본이 마스터로 승격한다.
4. 클라이언트 B가 같은 자원에 대해 A가 이미 점유하고 있는 락을 획득한다. -> 동시성 문제 발생!

특정 상황에서는 이와 같은 일이 문제가 되지 않을 수도 있습니다. 예를 들면, 장애 중 여러 클라이언트가 동시에 락을 점유하는 상황에서 말입니다.
이러한 사례에서는 복제본 기반의 솔루션을 사용해도 괜찮습니다. 그게 아니면 이 문서에 있는 솔루션을 도입하는 것을 추천드립니다.

## Correct Implementation with a Single Instance
***
단일 인스턴스 환경의 한계를 극복하기 전에, 이것을 어떻게 올바르게 구현할 수 있는지 먼저 살펴봅시다. 왜냐하면, 이 방법은
어느 정도의 경쟁 상태를 허용하는 애플리케이션에서는 실제로 유효한 해결책이며, 이후에 사용할 분산 락 알고리즘의 근간이 되기 때문입니다.

락을 획득하기 위해서는 다음과 같이 하면 됩니다:
~~~redis
    SET resource_name my_random_value NX PX 30000
~~~

키가 존재하지 않을 경우에만 키를 생성할 것이고 (NX 옵션), 30000 밀리초의 수명을 같습니다(PX 옵션). 키는 "my_random_value"로 값이 설정됩니다.
이 값은 모든 락 요청과 클라이언트에서 고유해야 합니다.

기본적으로 안전하게 락을 해제하기 위해 무작위한 값이 사용되고, 다음의 루아 스크립트를 통해 실행됩니다: 키가 존재할 경우에만 삭제하고. 내가 
기대하는 값과 정확히 일치하는 값을 가져야만 삭제하라는 뜻입니다.
~~~Lua Script
if redis.call("get",KEYS[1]) == ARGC[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
~~~

> Redis에서 Lua Script는 이벤트 루프 내 단일 명령어로 취급되기 때문에 중간에 다른 클라이언트가 끼어들 여지가 없다.

이것은 다른 클라이언트가 생성한 락을 삭제할 수 있기 때문에 중요합니다. 예를 들어, 어떤 클라이언트가 락을 획득하고, 작업을 수행하느라
오랫동안 블로킹되고, 그 사이 락의 유효 시간이 만료되어 다른 클라이언트가 락을 점유할 수 있습니다. 여기서 문제는 원래의 클라이언트가 락을 가지고
있어서 이를 해제하면, 이때 락을 가져간 다른 클라이언트의 락이 해제될 수 있습니다. `DEL`만 사용하는 것은 다른 클라이언트의 락을 해제할 수도 있기
때문에 안전하지 않습니다. 위의 Lua Script를 통하면, 각 락에 무작위 문자열이 포함되므로, 자신이 설정한 락일 때만 해제가 가능합니다.

그럼 무작의 문자열은 어떻게 설정해야 할까요? 기본적으로는 `/dev/urandom`을 통해 가져온 20바이트의 문자열이지만, 사용자의 작업에 알맞는
더 싼 비용의 방법이 있을수도 있습니다. 예를 들어, `/dev/urandom`으로 RC4를 시드하고, 그로부터 의사 난수 스트림을 생성할 수 있습니다. 더 간단한
방법으로는 마이크로초 정밀도의 UNIX 타임스탬프에 클라이언트 ID를 붙이는 방식이 있습니다. 완전하게 안전하지는 않지만, 대부분의 환경에서는 적합합니다.

락 유효 시간은 키의 유효 시간과 같습니다. 둘 다 자동으로 해제되며, 클라이언트가 락을 획득한 후 다른 클라이언트가 이를 다시 획득할 수 있기 전까지
상호 배제적으로 작업을 할 수 있는 시간입니다.

이제 락의 획득과 점유에 대한 좋은 방법이 있습니다. 이 시스템으로 단일 인스턴스로 구성된 고가용성의 비분산 시스템에 대해 안전하게 접근할 수 있습니다.
이제는 분산 시스템 환경에서 방안을 알아보도록 하겠습니다.

## The RedLock Algorithm
***
알고리즘의 분산 시스템 버전에서 N개의 Redis 마스터들이 존재한다고 가정합니다. 각 노드는 독립적이며, 어떠한 코디네이터 시스템과 복제본을 사용하지
않는 상태입니다. 단일 인스턴스에서 안전하게 락을 점유하고 획득하는 방법에 대해서는 이미 설명을 했습니다. 이 방법을 단일 인스턴스들에서는 그대로 
사용할 것입니다. 우리는 5개의 서버가 있다고 가정할 것이고, 각 서버를 별개의 컴퓨터 또는 가상 머신에 올려 독립적으로 장애가 발생할 수 있게 할 
것입니다. 

락을 획득하기 위해서는 클라이언트는 다음과 같이 수행해야 합니다:
1. 현재 시간을 밀리초로 가져옵니다.
2. 모든 N개의 인스턴스에서 같은 키와 같은 무작위 값을 가지고 락 획득을 시도합니다. 각 인스턴스에 락을 설정할 때, 클라이언트는 락의 자동 해제 시간에
비해 매우 짧은 타임아웃을 사용합니다. 예를 들어 자동 해제 시간이 10초면 타임아웃은 ~5-50 밀리초 정도로 설정합니다. 이것을 통해 클라이언트가
다운된 Redis 노드와 통신하는데 오랫동안 블로킹당하는 것을 방지합니다: 만약 인스턴스와 통신이 불가하다면 가능한 빨리 다른 인스턴스와 통신을 시도합니다.
3. 클라이언트는 1단계에서 가져온 타임스탬프에서 현재 시간을 락을 획득하는데 걸린 시간을 계산합니다. 클라이언트가 전체 인스턴스 중 과반수에서 
락을 획득했으며, 락을 획득하는 데 걸린 총 시간이 락 유효 시간보다 짧을 경우에만, 락이 획득된 것으로 간주합니다.
4. 만약 락을 획득했으면, 그 유효 시간은 초기 유효 시간에서 3단계에서 계산한 경과 시간을 뺀 값으로 계산합니다.
5. 만약 어떠한 이유로 클라이언트가 락을 획득하는데 실패한다면 (유효 시간이 음수거나, 과반수 이상에서 획득하지 못했을 때), 모든 인스턴스에서
락 해제를 시도합니다.

### Is the Algorithm Asynchronous?
이 알고리즘은 프로세스간 동기화된 시계는 없지만, 각 프로세스의 로컬 시간이 락의 자동 해제 시간에 비해 아주 작은 오차 범위 내에서 
거의 동일한 속도로 갱신된다는 가정에 기반합니다. 실제의 컴퓨터 환경에 매우 비슷합니다: 모든 컴퓨터는 로컬 시계를 가지고 있기, 그 시계들 간
clock drift는 매우 작습니다.

이 시점에서 상호 배제에 대해 다시 한번 정의할 필요가 있습니다: 락을 보유한 클라이언트가 3단계에서 얻은 락의 유효 시간 내에서 작업을 종료할 경우에만
보장됩니다. 다만, clock drift를 보정하기 위해 몇 밀리초를 빼야 합니다.

다음의 문서에서 clock drift가 제한되어야 하는 시스템에 대한 더 많은 정보를 볼 수 있습니다.
- [Leases: an efficient fault-tolerant mechanism for distributed file cache consistency](http://dl.acm.org/citation.cfm?id=74870)

### Retry on Failure
클라이언트가 락 획득에 실패하면, 무작위 시간 뒤에 다시 시도를 하게 됩니다. 이는 여러 클라이언트가 같은 자원에 대해 동시에 락을 획득하는 시도를
막기 위해서입니다 (동시에 시도해서 전부 실패하면 스플릿 브레인이 발생할 수도 있습니다). 클라이언트가 Redis 인스턴스의 과반수에 빠르게 락을 
시도할수록, 스플릿 브레인 상태가 발생할 수 있는 시간이 작아지고, 그에 따른 재시도 필요성도 줄어듭니다. 따라서 이상적으로는, 클라이언트가
멀티플렉싱을 통해 동시에 N개의 인스턴스에 `SET` 명령어를 보내는 것입니다.

클라이언트가 과반수 이상의 락을 획득하는 데 실패했을 때, 부분적으로 획득한 락들을 가능한 빨리 해제하는 것은 매우 중요합니다. 이렇게 해야 
락을 다시 점유하는데, 키가 만료되는 것을 기다리지 않아도 됩니다(네트워크 분할 같은 상황에서는 클라이언트가 Redis 인스턴스와 통신할 기회가 없기
때문에 어쩔 수 없이 키 만료 시간을 기다려야 합니다).

### Releasing the Lock
락을 해제하는 것은 간단합니다. 클라이언트가 해당 인스턴스에서 락의 획득 여부에 상관없이 수행할 수 있습니다.

### Safety Arguments
이 알고리즘이 안전할까요? 다른 시나리오에서는 어떤 일이 일어날 수 있는지 알아봅시다.

클라이언트가 과반수 이상에서 락을 획득했다고 가정해봅시다. 모든 인스턴스에서 같은 만료 시간을 가진 키들이 존재할 것입니다. 하지만, 키가 다른
시간에 설정되었다면, 만료도 각각 다른 시간에 이루어질 것입니다. 첫 번째 키가 최악의 경우인 시간 T1(첫 서버에 요청을 보내기 전 측정한 시간)에
설정되고, 마지막 키가 T2(마지막 서버에서 응답을 받은 시간)에 설정되었다면, 첫 키가 최소 다음 시간동안 생존한다는 것을 보장할 수 있습니다.
`MIN_VALIDITY = TTL - (T2 - T1) - CLOCK_DRIFT` 나머지 키들은 이보다 더 늦게 만료되므로, 모든 키들이 동시에 존재하는 시간이 `MIN_VALDITY`만큼
인 것을 보장할 수 있습니다.

과반수 이상의 키가 설정되어 있는 동안에는 다른 클라이언트가 락을 획득할 수 없습니다. `N/2+1 SET NX` 연산이 N/2+1만큼 키가 존재하면
성공할 수 없기 때문입니다. 따라서, 락이 획득되었다면, 같은 시간에 다시 획득하는 것은 불가능합니다.

하지만, 동시에 여러 클라이언트가 락을 획득하려고 할 때, 모두 동시에 성공하지 못하게 보장해야 합니다.

어떤 클라이언트가 락의 최대 유효 시간 이상만큼 시간을 들여 과반수의 인스턴스에 락을 획득했다면, 락을 무효화시키고, 모든 인스턴스에서
락을 해제할 것입니다. 그래서 락의 최대 유효 시간 미만일 때만 고려하면 됩니다. 이 경우에서는, MIN_VALIDITY 동안은 어떤 클라이언트도 다시 
락을 획득할 수 없어야 합니다. 즉, 여러 클라이언트가 과반수 이상의 락을 획득하는 경우는 이에 걸린 시간이 TTL보다 큰 경우 다른 말로, 락이 무효
상태일 때만 가능합니다.

### Liveness Arguments
시스템의 생존성은 다음의 3가지 요소에 기반합니다:
1. 락의 자동 해제: 락이 자동으로 만료됨으로서 락을 다시 획득할 수 있습니다.
2. 클라이언트들이 락을 획득하지 못했거나, 락을 획득한 후 작업을 완료했을 때, 락을 자발적으로 해제합니다. 이에 락을 다시 잡기 위해 키가 만료되는
것을 기다라지 않아도 됩니다.
3. 클라이언트가 락 획득을 재시도하면, 이는 과반수의 락을 획득하는 것보다 비교적 오래걸리는 작업입니다. 이렇게 함으로써 경쟁 상태에서
스플릿 브레인 현상이 발생할 확률을 낮춥니다.

하지만 네트워크 분할이 발생하면, 우리는 TTL 시간만큼의 가용성 손해를 감수해야 합니다. 만약 네트워크 분할이 지속적으로 발생하면, 이 손해는
무기한으로 발생하게 됩니다. 이러한 상황은 클라이언트가 락을 획득한 직후 네트워크에서 분리되어 락을 해제하지 못할 때마다 발생합니다.

기본적으로, 무한히 지속되는 네트워크 분할이 발생한다면, 시스템은 무한한 시간 동안 가용하지 않게 될 수도 있습니다.

### Performance, Crash Recovery and fsync
Redis를 락 서버로서 사용하는 많은 사용자들은 락을 획득하고 해제하는 지연 시간과 초당 락 획득/해제 throughput에서 높은 성능을 요구합니다.
이러한 요구사항을 만족하기 위해서 N개의 Redis 서버와 통신할 때 지연 시간을 줄이기 위해 멀티플렉싱이 확실히 적합한 전략입니다 (소켓을 논블로킹
모드로 설정하고, 모든 명령을 먼저 전송한 뒤, 나중에 모든 응답을 한꺼번에 읽는 방식. 클라이언트와 각 Redis 인스턴스 간의 RTT가 비슷하다는 전제).

하지만, 충돌-복구 시스템 모델을 목표로 한다면, 지속성에 대한 또 다른 고려사항이 존재합니다.

Redis를 전혀 지속성을 고려하지 않은 채로 사용한다고 가정해봅시다. 어떤 클라이언트가 5개의 인스턴스 중 3개의 인스턴스에서 락을 획득했습니다.
클라이언트가 락을 획득한 하나의 인스턴스가 재시작을 하게 되면, 다른 클라이언트가 락을 획득할 수 있는 3개의 인스턴스가 존재하게 되는 겁니다. 
이렇게 되면 락의 기능을 상실하게 됩니다.

AOF를 활성화하면, 좀 더 나아질 것입니다. 예를 들어, `SHUTDOWN`을 통해 서버를 업그레이드하고 재시작할 수 있습니다. Redis의 만료는 서버가
오프라인인 상태에도 시간이 계속 흐르는 의미론적 동작을 하기 때문에 요구사항을 잘 충족시킵니다. 하지만, 이런 것은 전부 정상 종료일때만 해당됩니다.
만약 정전이 발생하면요? Redis가 기본 설정대로 1초마다 디스크에 `fsync`를 수행하도록 설정되었다면, 재시작 후 키가 유실될 수 있습니다. 이론적으로,
어떤 종류의 재시작에도 락의 안전을 보장하고 싶으면, `fsync=always` 영속화 설정을 해야합니다. 다만 이 경우, 매번 디스크에 동기화해야 하기 때문에
성능 저하가 발생할 수 있습니다(성능 vs 일관성).

하지만, 처음보다는 나아진 상황입니다. 기본적으로 인스턴스가 다운 후 재시작 후에, 현재 활성화된 어떤 락에도 더 이상 참여하지 않는다면 이 알고리즘의
안전성은 보장됩니다. 다시 말해, 인스턴스가 시스템에 다시 합류할 때 다른 인스턴스들에서 락이 획득되었고, 유효한 상태라면 문제가 없다는 뜻입니다.

이것을 보장하기 위해 그냥 인스턴스를 장애 후에, 최대 TTL동안만 비활성화시키면 됩니다. 이 시간이 지나면 모든 키들이 만료가 될 것이니깐요.

지연 재시작을 사용하면, Redis의 영속화 전략없이도 안정성을 보장할 수 있습니다. 하지만, 이 방식은 가용성 측면에서 손해가 발생할 수 있습니다.
예를 들어, 과반수의 인스턴스에서 장애가 발생했을 경우, 시스템 전체가 TTL동안 비활성화된다는 것입니다. 다른 말로, 이 시간동안 어떤 자원에
대해서도 락을 획득할 수 없다는 뜻입니다.

### Making the algorithm more reliable: Extending the lock
클라이언트가 수행하는 작업이 작은 단계로 구성되어있다며, 짧은 유효 시간을 가진 락을 기본으로 사용할 수 있으며, 락 연장 기능을 구현해 알고리즘을
확장하는 것이 가능합니다. 클라이언트가 연산 중에 락의 만료시간이 다가오면, 모든 인스턴스에 Lua Script를 보내 락을 연장할 수 있습니다. 이때,
키가 존재해야 하고, 해당 무작위 값이 할당받은 클라이언트의 값하고 동일해야 합니다.

클라이언트는 락을 다시 획득했다고 간주하기 위해서는, 과반수 인스턴스에서 락을 성공적으로 연장했으며, 유효 시간 내에 해당 연장이 완료되었을 
경우에만 그렇게 간주해야 합니다. 기본적으로, 락을 획득할 때 사용했던 알고리즘과 매우 유사한 방식을 사용해야 합니다.

하지만 이는 기술적으로 알고리즘을 변경하는 것은 아니므로, 락 재획득 시도 횟수는 제한되어야 합니다. 그렇지 않으면 생존성 중 하나가 위배될 수 
있습니다.

### Disclaimer about consistency
이 페이지의 마지막에 있는 Analysis of Redlock 섹션을 반드시 면밀히 검토해보길 권장합니다. Martin Kleppmann의 글과 이에 대한 
antirez의 반론은 매우 중요한 자료들입니다.

1. 펜싱 토큰을 구현해야 합니다. 이는 처리 시간이 길어질 수 있는 프로세스에 특히 중요하며, 모든 분산 락 시스템에 적용되는 원칙입니다. 
락의 수명을 연장하는 것도 하나의 방법이지만, 락을 획득한 프로세스가 살아 있다는 이유만으로 해당 락이 유지된다고 가정해서는 안 됩니다.
2. Redis는 TTL 만료 메커니즘에 단조 시계를 사용하지 않습니다. 이것은 wall-clock shift가 하나 이상의 프로세스가 락을 획득할 수 있게 할 수도
있다는 것입니다. 이 문제는 시스템 관리자가 서버 시간을 수동으로 변경하지 못하도록 하고, Network Time Protocol를 적절히 설정함으로써 완화할 
수는 있지만, 현실에서는 여전히 이러한 clock drift가 발생할 가능성이 존재하며, 이로 인해 일관성이 위협받을 수 있습니다.

## Want to help?
***
분산 시스템에 관심이 있다면, 이 알고리즘에 대한 당신의 의견이나 분석을 공유하는 것은 매우 가치 있는 일이 될 것입니다. 또한, 여러 언어로
번역해주시면 매우 좋을 것입니다.

### Analysis of Redlock
***
1. Martin Kleppmann께서 분석한 글입니다. 이거에 대한 반론은 다음에서 볼 수 있습니다.
- Martine Kleppmann: http://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html
  - RedLock이 분산 락에 적합하지 않다는 분석
- Antirez: http://antirez.com/news/101
  - 마틴 클레프만의 분석에 대한 반론